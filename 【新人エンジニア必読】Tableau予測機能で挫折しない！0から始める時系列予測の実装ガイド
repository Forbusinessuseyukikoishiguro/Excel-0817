# ã€æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å¿…èª­ã€‘Tableauäºˆæ¸¬æ©Ÿèƒ½ã§æŒ«æŠ˜ã—ãªã„ï¼0ã‹ã‚‰å§‹ã‚ã‚‹æ™‚ç³»åˆ—äºˆæ¸¬ã®å®Ÿè£…ã‚¬ã‚¤ãƒ‰

ã“ã‚“ã«ã¡ã¯ï¼æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®çš†ã•ã‚“ã€ã€ŒTableauã§äºˆæ¸¬åˆ†æã‚„ã£ã¦ã€ã¨è¨€ã‚ã‚Œã¦å›°ã£ãŸã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ

ç§ã‚‚æœ€åˆã¯ã€Œäºˆæ¸¬ã£ã¦ä½•ï¼Ÿã€ã€Œæ™‚ç³»åˆ—ã£ã¦é›£ã—ãã†...ã€ã¨æ€ã£ã¦ã„ã¾ã—ãŸãŒã€3ãƒ¶æœˆã§Tableauäºˆæ¸¬ã®ãƒ—ãƒ­ã«ãªã‚Šã¾ã—ãŸï¼ä»Šå›ã¯ã€åŒã˜æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®ç›®ç·šã§ã€æŒ«æŠ˜ã—ãªã„äºˆæ¸¬åˆ†æã®å§‹ã‚æ–¹ã‚’ãŠä¼ãˆã—ã¾ã™ ğŸš€

## ğŸ¯ ã“ã®è¨˜äº‹ã‚’èª­ã‚€ã¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨

- **Tableauäºˆæ¸¬æ©Ÿèƒ½**ã®åŸºç¤ã‹ã‚‰å®Ÿè£…ã¾ã§
- **ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è¦–ç‚¹**ã§ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- **è‡ªå‹•åŒ–**ã‚’æ„è­˜ã—ãŸäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
- **APIé€£æº**ã§ã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å–å¾—
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

## ğŸ¤” ãªãœã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒäºˆæ¸¬åˆ†æã‚’å­¦ã¶ã¹ãã‹

### ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã®å‰µå‡º
```
å¾“æ¥ã®ã‚·ã‚¹ãƒ†ãƒ ï¼šã€Œéå»ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã€
äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ï¼šã€Œæœªæ¥ã®è¡Œå‹•æŒ‡é‡ã‚’æä¾›ã€
â†’ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®ä¾¡å€¤ãŒæ ¼æ®µã«ã‚¢ãƒƒãƒ—ï¼
```

### ã‚­ãƒ£ãƒªã‚¢ã®åºƒãŒã‚Š
- ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢

### æŠ€è¡“ã‚¹ã‚­ãƒ«ã®å‘ä¸Š
- çµ±è¨ˆçš„æ€è€ƒåŠ›
- ãƒ‡ãƒ¼ã‚¿å‡¦ç†èƒ½åŠ›
- ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆåŠ›
- ãƒ“ã‚¸ãƒã‚¹ç†è§£åŠ›

## ğŸ“Š Tableauäºˆæ¸¬æ©Ÿèƒ½ã®æŠ€è¡“çš„ç†è§£

### å†…éƒ¨ã§å‹•ã„ã¦ã„ã‚‹äºˆæ¸¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**1. æŒ‡æ•°å¹³æ»‘åŒ–æ³•ï¼ˆExponential Smoothingï¼‰**
```
åŸºæœ¬å¼ï¼š
S[t] = Î± Ã— X[t] + (1-Î±) Ã— S[t-1]

Î±: å¹³æ»‘åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0-1ï¼‰
X[t]: æ™‚åˆ»tã®å®Ÿæ¸¬å€¤
S[t]: æ™‚åˆ»tã®å¹³æ»‘åŒ–å€¤
```

**2. ARIMAï¼ˆè‡ªå·±å›å¸°å’Œåˆ†ç§»å‹•å¹³å‡ï¼‰**
```
ARIMA(p,d,q)
p: è‡ªå·±å›å¸°é …æ•°
d: å·®åˆ†å›æ•°
q: ç§»å‹•å¹³å‡é …æ•°
```

**3. Tableauã®è‡ªå‹•é¸æŠ**
```
ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã‚’åˆ†æ
â†“
æœ€é©ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è‡ªå‹•é¸æŠ
â†“
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–
â†“
äºˆæ¸¬çµæœã®å‡ºåŠ›
```

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã¨åˆ¶ç´„

**ãƒ‡ãƒ¼ã‚¿è¦ä»¶**
```json
{
  "minimum_data_points": 9,
  "maximum_data_points": "unlimited",
  "date_field": "required",
  "measure_field": "required (numeric)",
  "null_tolerance": "< 30%"
}
```

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ¶ç´„**
```
ãƒ‡ãƒ¼ã‚¿é‡ < 100ä¸‡è¡Œ: é«˜é€Ÿ
ãƒ‡ãƒ¼ã‚¿é‡ 100ä¸‡-500ä¸‡è¡Œ: ä¸­é€Ÿ
ãƒ‡ãƒ¼ã‚¿é‡ > 500ä¸‡è¡Œ: ä½é€Ÿï¼ˆè¦æœ€é©åŒ–ï¼‰
```

## ğŸ”§ å®Ÿè£…æ‰‹é †ï¼šã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—

### Step 1: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­è¨ˆ

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒä¾‹**
```sql
-- å£²ä¸Šãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE sales_data (
    id SERIAL PRIMARY KEY,
    date_recorded DATE NOT NULL,
    sales_amount DECIMAL(10,2) NOT NULL,
    product_category VARCHAR(50),
    region VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
CREATE INDEX idx_sales_date ON sales_data(date_recorded);
CREATE INDEX idx_sales_category ON sales_data(product_category);
```

**ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ç”¨SQL**
```sql
-- æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
SELECT 
    COUNT(*) as total_records,
    COUNT(date_recorded) as date_count,
    COUNT(sales_amount) as sales_count,
    (COUNT(*) - COUNT(sales_amount)) * 100.0 / COUNT(*) as missing_percentage
FROM sales_data;

-- ç•°å¸¸å€¤æ¤œå‡º
WITH stats AS (
    SELECT 
        AVG(sales_amount) as mean_sales,
        STDDEV(sales_amount) as std_sales
    FROM sales_data
)
SELECT *
FROM sales_data, stats
WHERE sales_amount > (mean_sales + 3 * std_sales)
   OR sales_amount < (mean_sales - 3 * std_sales);
```

### Step 2: Tableauæ¥ç¶šè¨­å®š

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®æœ€é©åŒ–**
```json
{
  "connection_type": "live",
  "extract_refresh": "daily",
  "row_limit": 1000000,
  "timeout": 300,
  "ssl": true,
  "connection_pooling": true
}
```

**ã‚«ã‚¹ã‚¿ãƒ SQLæ´»ç”¨**
```sql
-- é›†è¨ˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§é«˜é€ŸåŒ–
SELECT 
    DATE_TRUNC('day', date_recorded) as date_day,
    SUM(sales_amount) as daily_sales,
    COUNT(*) as transaction_count,
    product_category
FROM sales_data
WHERE date_recorded >= CURRENT_DATE - INTERVAL '2 years'
GROUP BY DATE_TRUNC('day', date_recorded), product_category
ORDER BY date_day;
```

### Step 3: äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…

**åŸºæœ¬çš„ãªäºˆæ¸¬ã®ä½œæˆ**
```
1. æ—¥ä»˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åˆ—ã‚·ã‚§ãƒ«ãƒ•ã«ãƒ‰ãƒ©ãƒƒã‚°
   â†’ è‡ªå‹•çš„ã«å¹´-æœˆã«é›†è¨ˆã•ã‚Œã‚‹
   
2. å£²ä¸Šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¡Œã‚·ã‚§ãƒ«ãƒ•ã«ãƒ‰ãƒ©ãƒƒã‚°
   â†’ è‡ªå‹•çš„ã«SUM(å£²ä¸Š)ã«ãªã‚‹
   
3. åˆ†æãƒšã‚¤ãƒ³ã‹ã‚‰ã€Œäºˆæ¸¬ã€ã‚’ãƒ‰ãƒ©ãƒƒã‚°
   â†’ äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ãŒè¡¨ç¤º
```

**é«˜åº¦ãªè¨­å®š**
```json
{
  "forecast_length": 6,
  "forecast_units": "months",
  "model_type": "automatic",
  "ignore_last_periods": 0,
  "seasonal_pattern": "automatic",
  "quality": "balanced"
}
```

### Step 4: APIã‚’ä½¿ã£ãŸè‡ªå‹•åŒ–

**Tableau REST APIæ´»ç”¨**
```python
import requests
import json

class TableauForecastAPI:
    def __init__(self, server_url, username, password, site_id=''):
        self.server_url = server_url
        self.username = username
        self.password = password
        self.site_id = site_id
        self.auth_token = None
        
    def sign_in(self):
        """Tableau Serverã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³"""
        signin_url = f"{self.server_url}/api/3.9/auth/signin"
        
        payload = {
            'credentials': {
                'name': self.username,
                'password': self.password,
                'site': {'contentUrl': self.site_id}
            }
        }
        
        response = requests.post(signin_url, json=payload)
        
        if response.status_code == 200:
            self.auth_token = response.json()['credentials']['token']
            return True
        return False
    
    def refresh_extract(self, datasource_id):
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’æ›´æ–°"""
        headers = {'X-Tableau-Auth': self.auth_token}
        refresh_url = f"{self.server_url}/api/3.9/sites/{self.site_id}/datasources/{datasource_id}/refresh"
        
        response = requests.post(refresh_url, headers=headers)
        return response.status_code == 202
    
    def get_view_data(self, view_id):
        """ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        headers = {'X-Tableau-Auth': self.auth_token}
        data_url = f"{self.server_url}/api/3.9/sites/{self.site_id}/views/{view_id}/data"
        
        response = requests.get(data_url, headers=headers)
        return response.content

# ä½¿ç”¨ä¾‹
api = TableauForecastAPI("https://your-server.com", "username", "password")
api.sign_in()
api.refresh_extract("datasource-id")
forecast_data = api.get_view_data("forecast-view-id")
```

### Step 5: äºˆæ¸¬ç²¾åº¦ã®ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

**Python ã§ã®ç²¾åº¦è¨ˆç®—**
```python
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error

class ForecastAccuracyMonitor:
    def __init__(self, actual_data, forecast_data):
        self.actual = actual_data
        self.forecast = forecast_data
        
    def calculate_mape(self):
        """MAPEï¼ˆå¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®ï¼‰ã‚’è¨ˆç®—"""
        return mean_absolute_percentage_error(self.actual, self.forecast) * 100
    
    def calculate_rmse(self):
        """RMSEï¼ˆå¹³æ–¹æ ¹å¹³å‡äºŒä¹—èª¤å·®ï¼‰ã‚’è¨ˆç®—"""
        return np.sqrt(mean_squared_error(self.actual, self.forecast))
    
    def calculate_accuracy_score(self):
        """ç²¾åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        mape = self.calculate_mape()
        if mape <= 10:
            return "Excellent"
        elif mape <= 20:
            return "Good"
        elif mape <= 50:
            return "Acceptable"
        else:
            return "Poor"
    
    def generate_report(self):
        """ç²¾åº¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = {
            "mape": self.calculate_mape(),
            "rmse": self.calculate_rmse(),
            "accuracy_score": self.calculate_accuracy_score(),
            "data_points": len(self.actual),
            "timestamp": pd.Timestamp.now()
        }
        return report

# ä½¿ç”¨ä¾‹
monitor = ForecastAccuracyMonitor(actual_sales, predicted_sales)
accuracy_report = monitor.generate_report()
print(f"MAPE: {accuracy_report['mape']:.2f}%")
```

## ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒ™ãƒ«ã§ã®æœ€é©åŒ–

**äº‹å‰é›†è¨ˆæˆ¦ç•¥**
```sql
-- æ—¥æ¬¡é›†è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
CREATE TABLE daily_sales_summary AS
SELECT 
    date_recorded,
    product_category,
    SUM(sales_amount) as total_sales,
    COUNT(*) as transaction_count,
    AVG(sales_amount) as avg_transaction_value
FROM sales_data
GROUP BY date_recorded, product_category;

-- å®šæœŸå®Ÿè¡Œã®ãŸã‚ã®ã‚¹ãƒˆã‚¢ãƒ‰ãƒ—ãƒ­ã‚·ãƒ¼ã‚¸ãƒ£
CREATE OR REPLACE FUNCTION refresh_daily_summary()
RETURNS void AS $$
BEGIN
    DELETE FROM daily_sales_summary 
    WHERE date_recorded >= CURRENT_DATE - INTERVAL '7 days';
    
    INSERT INTO daily_sales_summary
    SELECT 
        date_recorded,
        product_category,
        SUM(sales_amount),
        COUNT(*),
        AVG(sales_amount)
    FROM sales_data
    WHERE date_recorded >= CURRENT_DATE - INTERVAL '7 days'
    GROUP BY date_recorded, product_category;
END;
$$ LANGUAGE plpgsql;
```

**ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–**
```sql
-- è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
CREATE INDEX idx_sales_date_category ON sales_data(date_recorded, product_category);

-- éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ¡ä»¶ä»˜ãï¼‰
CREATE INDEX idx_recent_sales ON sales_data(date_recorded)
WHERE date_recorded >= CURRENT_DATE - INTERVAL '1 year';
```

### 2. Tableauè¨­å®šæœ€é©åŒ–

**ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ãƒˆæœ€é©åŒ–**
```json
{
  "extract_settings": {
    "aggregation": "visible_dimensions",
    "filters": ["date_recorded >= '2022-01-01'"],
    "top_n": 1000000,
    "sampling": "none",
    "rollup": true,
    "materialize_calculations": true
  }
}
```

**è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æœ€é©åŒ–**
```
// âŒ éåŠ¹ç‡ãªè¨ˆç®—
IF [Sales] > 1000000 THEN "High"
ELSEIF [Sales] > 500000 THEN "Medium"
ELSE "Low" END

// âœ… åŠ¹ç‡çš„ãªè¨ˆç®—
IF [Sales] > 1000000 THEN "High"
ELSEIF [Sales] > 500000 THEN "Medium"
ELSE "Low" END

// ã•ã‚‰ã«æœ€é©åŒ–ï¼šLODè¨ˆç®—ã‚’æ´»ç”¨
{FIXED [Date] : SUM([Sales])}
```

### 3. ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥**
```python
import redis
import pickle
from datetime import datetime, timedelta

class ForecastCache:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port)
        
    def set_forecast(self, key, forecast_data, ttl_hours=24):
        """äºˆæ¸¬çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        serialized_data = pickle.dumps(forecast_data)
        ttl_seconds = ttl_hours * 3600
        self.redis_client.setex(key, ttl_seconds, serialized_data)
    
    def get_forecast(self, key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰äºˆæ¸¬çµæœã‚’å–å¾—"""
        cached_data = self.redis_client.get(key)
        if cached_data:
            return pickle.loads(cached_data)
        return None
    
    def invalidate_forecast(self, pattern):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–"""
        keys = self.redis_client.keys(pattern)
        if keys:
            self.redis_client.delete(*keys)

# ä½¿ç”¨ä¾‹
cache = ForecastCache()
forecast_key = "sales_forecast_2024_category_A"
cached_forecast = cache.get_forecast(forecast_key)

if not cached_forecast:
    # æ–°ã—ã„äºˆæ¸¬ã‚’è¨ˆç®—
    new_forecast = calculate_forecast()
    cache.set_forecast(forecast_key, new_forecast, ttl_hours=12)
```

## ğŸ”„ CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®äºˆæ¸¬è‡ªå‹•åŒ–

### GitHub Actions ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹

```yaml
name: Tableau Forecast Pipeline

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯æ—¥åˆå‰2æ™‚ã«å®Ÿè¡Œ
  workflow_dispatch:

jobs:
  update-forecasts:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Data Quality Check
      run: |
        python scripts/data_quality_check.py
    
    - name: Refresh Tableau Extract
      env:
        TABLEAU_SERVER: ${{ secrets.TABLEAU_SERVER }}
        TABLEAU_USERNAME: ${{ secrets.TABLEAU_USERNAME }}
        TABLEAU_PASSWORD: ${{ secrets.TABLEAU_PASSWORD }}
      run: |
        python scripts/refresh_tableau_extract.py
    
    - name: Validate Forecast Accuracy
      run: |
        python scripts/validate_forecast.py
    
    - name: Send Notification
      if: failure()
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### Docker ã‚³ãƒ³ãƒ†ãƒŠåŒ–

```dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY scripts/ ./scripts/
COPY config/ ./config/

# Tableau Python Server API ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN pip install tableauserverclient

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
ENV PYTHONPATH=/app

CMD ["python", "scripts/forecast_pipeline.py"]
```

## ğŸ“Š ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

### 1. ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import pandas as pd

app = FastAPI(title="Forecast Service API")

class ForecastRequest(BaseModel):
    data_source: str
    forecast_periods: int
    confidence_level: float = 0.95
    
class ForecastResponse(BaseModel):
    forecast_values: List[float]
    confidence_intervals: List[dict]
    accuracy_metrics: dict
    timestamp: str

@app.post("/api/v1/forecast", response_model=ForecastResponse)
async def create_forecast(request: ForecastRequest):
    """äºˆæ¸¬APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        data = get_data_from_source(request.data_source)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        forecast_result = run_tableau_forecast(
            data, 
            periods=request.forecast_periods,
            confidence=request.confidence_level
        )
        
        return ForecastResponse(**forecast_result)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {"status": "healthy", "service": "forecast-api"}
```

### 2. ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```python
import asyncio
import aioredis
import json
from datetime import datetime

class ForecastEventHandler:
    def __init__(self, redis_url):
        self.redis_url = redis_url
        
    async def handle_data_update_event(self, event_data):
        """ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
        try:
            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹
            update_info = json.loads(event_data)
            
            # å½±éŸ¿ã®ã‚ã‚‹äºˆæ¸¬ã‚’ç‰¹å®š
            affected_forecasts = self.identify_affected_forecasts(update_info)
            
            # äºˆæ¸¬ã‚’å†è¨ˆç®—
            for forecast_id in affected_forecasts:
                await self.regenerate_forecast(forecast_id)
                
            # å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ
            await self.publish_event("forecast.updated", {
                "forecasts": affected_forecasts,
                "timestamp": datetime.utcnow().isoformat()
            })
            
        except Exception as e:
            await self.publish_event("forecast.error", {
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            })
    
    async def publish_event(self, event_type, event_data):
        """ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ"""
        redis = await aioredis.from_url(self.redis_url)
        await redis.publish(event_type, json.dumps(event_data))
        await redis.close()
```

## âš¡ æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒé™¥ã‚Šã‚„ã™ã„ç½ ã¨å¯¾ç­–

### 1. ãƒ‡ãƒ¼ã‚¿å‹ã®è½ã¨ã—ç©´

**âŒ ã‚ˆãã‚ã‚‹é–“é•ã„**
```sql
-- æ—¥ä»˜ãŒæ–‡å­—åˆ—å‹ã§æ ¼ç´ã•ã‚Œã¦ã„ã‚‹
SELECT date_column FROM sales_data
WHERE date_column = '2024-01-01';  -- æ–‡å­—åˆ—æ¯”è¼ƒã«ãªã£ã¦ã—ã¾ã†
```

**âœ… æ­£ã—ã„å¯¾å‡¦æ³•**
```sql
-- æ—¥ä»˜å‹ã«å¤‰æ›ã—ã¦ã‹ã‚‰å‡¦ç†
SELECT TO_DATE(date_column, 'YYYY-MM-DD') FROM sales_data
WHERE TO_DATE(date_column, 'YYYY-MM-DD') = DATE '2024-01-01';

-- ã¾ãŸã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ™‚ã«é©åˆ‡ãªå‹ã‚’æŒ‡å®š
ALTER TABLE sales_data 
ALTER COLUMN date_column TYPE DATE 
USING TO_DATE(date_column, 'YYYY-MM-DD');
```

### 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è½ã¨ã—ç©´

**âŒ ã‚ˆãã‚ã‚‹é–“é•ã„**
```python
# å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€
df = pd.read_sql("SELECT * FROM large_table", connection)
```

**âœ… æ­£ã—ã„å¯¾å‡¦æ³•**
```python
# ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†
chunk_size = 10000
for chunk in pd.read_sql("SELECT * FROM large_table", 
                         connection, chunksize=chunk_size):
    process_chunk(chunk)
```

### 3. äºˆæ¸¬ã®è§£é‡ˆãƒŸã‚¹

**âŒ ã‚ˆãã‚ã‚‹é–“é•ã„**
```
ã€Œ95%ä¿¡é ¼åŒºé–“ã€= 95%ã®ç¢ºç‡ã§å½“ãŸã‚‹
```

**âœ… æ­£ã—ã„ç†è§£**
```
ã€Œ95%ä¿¡é ¼åŒºé–“ã€= åŒã˜æ¡ä»¶ã§100å›äºˆæ¸¬ã—ãŸå ´åˆã€
95å›ã¯ã“ã®ç¯„å›²ã«å®Ÿéš›ã®å€¤ãŒå…¥ã‚‹
```

## ğŸ¯ å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼šå£²ä¸Šäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
```
ç›®æ¨™ï¼šECã‚µã‚¤ãƒˆã®æ—¥æ¬¡å£²ä¸Šã‚’7æ—¥å…ˆã¾ã§äºˆæ¸¬
æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ï¼š
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼šPostgreSQL
- äºˆæ¸¬ï¼šTableau
- APIï¼šFastAPI
- ç›£è¦–ï¼šGrafana
- ãƒ‡ãƒ—ãƒ­ã‚¤ï¼šDocker + Kubernetes
```

### å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—

**Week 1: ãƒ‡ãƒ¼ã‚¿åŸºç›¤æ§‹ç¯‰**
```sql
-- 1. åŸºæœ¬ãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆ
CREATE TABLE sales_transactions (
    id BIGSERIAL PRIMARY KEY,
    transaction_date DATE NOT NULL,
    product_id INTEGER NOT NULL,
    category_id INTEGER NOT NULL,
    sales_amount DECIMAL(10,2) NOT NULL,
    quantity INTEGER NOT NULL,
    customer_segment VARCHAR(20),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 2. é›†è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆ
CREATE TABLE daily_sales_agg (
    sales_date DATE PRIMARY KEY,
    total_sales DECIMAL(12,2) NOT NULL,
    transaction_count INTEGER NOT NULL,
    avg_order_value DECIMAL(8,2) NOT NULL,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 3. è‡ªå‹•é›†è¨ˆã®ãƒˆãƒªã‚¬ãƒ¼è¨­å®š
CREATE OR REPLACE FUNCTION update_daily_agg()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO daily_sales_agg (sales_date, total_sales, transaction_count, avg_order_value)
    SELECT 
        NEW.transaction_date,
        SUM(sales_amount),
        COUNT(*),
        AVG(sales_amount)
    FROM sales_transactions 
    WHERE transaction_date = NEW.transaction_date
    ON CONFLICT (sales_date) DO UPDATE SET
        total_sales = EXCLUDED.total_sales,
        transaction_count = EXCLUDED.transaction_count,
        avg_order_value = EXCLUDED.avg_order_value,
        updated_at = CURRENT_TIMESTAMP;
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

**Week 2: Tableauäºˆæ¸¬å®Ÿè£…**
```python
# Tableau Server Client ã‚’ä½¿ã£ãŸè‡ªå‹•åŒ–
import tableauserverclient as TSC

class TableauForecastAutomation:
    def __init__(self, server_url, username, password, site_id=''):
        self.server = TSC.Server(server_url)
        self.username = username
        self.password = password
        self.site_id = site_id
        
    def create_forecast_workbook(self, datasource_id):
        """äºˆæ¸¬ãƒ¯ãƒ¼ã‚¯ãƒ–ãƒƒã‚¯ã‚’è‡ªå‹•ç”Ÿæˆ"""
        with self.server.auth.sign_in(TSC.TableauAuth(self.username, self.password, self.site_id)):
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            datasource = self.server.datasources.get_by_id(datasource_id)
            
            # ãƒ¯ãƒ¼ã‚¯ãƒ–ãƒƒã‚¯ã‚’ä½œæˆ
            wb = TSC.WorkbookItem(name='Sales Forecast', project_id='your-project-id')
            wb = self.server.workbooks.publish(wb, 'path/to/forecast_template.twbx', 'Overwrite')
            
            return wb.id
```

**Week 3: APIé–‹ç™º**
```python
from fastapi import FastAPI, BackgroundTasks
from sqlalchemy import create_engine
import pandas as pd

app = FastAPI()

@app.post("/api/forecast/trigger")
async def trigger_forecast_update(background_tasks: BackgroundTasks):
    """äºˆæ¸¬æ›´æ–°ã‚’ãƒˆãƒªã‚¬ãƒ¼"""
    background_tasks.add_task(update_forecast_pipeline)
    return {"message": "Forecast update started"}

@app.get("/api/forecast/latest")
async def get_latest_forecast():
    """æœ€æ–°ã®äºˆæ¸¬çµæœã‚’å–å¾—"""
    engine = create_engine(DATABASE_URL)
    
    query = """
    SELECT forecast_date, predicted_sales, confidence_lower, confidence_upper
    FROM forecast_results 
    WHERE created_at = (SELECT MAX(created_at) FROM forecast_results)
    ORDER BY forecast_date
    """
    
    df = pd.read_sql(query, engine)
    return df.to_dict(orient='records')
```

**Week 4: ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ**
```python
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
forecast_accuracy_gauge = Gauge('forecast_accuracy_mape', 'Forecast accuracy (MAPE)')
forecast_requests_counter = Counter('forecast_requests_total', 'Total forecast requests')
forecast_duration_histogram = Histogram('forecast_duration_seconds', 'Forecast calculation duration')

class ForecastMonitoring:
    def __init__(self):
        self.accuracy_threshold = 20.0  # MAPE 20%ä»¥ä¸‹ãŒç›®æ¨™
        
    def check_accuracy(self, actual_values, predicted_values):
        """ç²¾åº¦ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°"""
        mape = calculate_mape(actual_values, predicted_values)
        forecast_accuracy_gauge.set(mape)
        
        if mape > self.accuracy_threshold:
            self.send_alert(f"Forecast accuracy degraded: MAPE={mape:.2f}%")
    
    def send_alert(self, message):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡"""
        # Slack, ãƒ¡ãƒ¼ãƒ«ç­‰ã§ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
        pass
```

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼šã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### ãƒ¬ãƒ™ãƒ«1ï¼šåŸºç¤ç¿’å¾—ï¼ˆ1-2ãƒ¶æœˆï¼‰
- [ ] SQL ã®åŸºæœ¬æ“ä½œ
- [ ] Tableau ã®åŸºæœ¬æ“ä½œ
- [ ] æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç†è§£
- [ ] åŸºæœ¬çš„ãªäºˆæ¸¬ã®ä½œæˆ

### ãƒ¬ãƒ™ãƒ«2ï¼šå®Ÿè£…åŠ›å‘ä¸Šï¼ˆ2-3ãƒ¶æœˆï¼‰
- [ ] API ã‚’ä½¿ã£ãŸè‡ªå‹•åŒ–
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- [ ] ãƒ†ã‚¹ãƒˆã®æ›¸ãæ–¹

### ãƒ¬ãƒ™ãƒ«3ï¼šã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆï¼ˆ3-6ãƒ¶æœˆï¼‰
- [ ] ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹è¨­è¨ˆ
- [ ] CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- [ ] ç›£è¦–ãƒ»ãƒ­ã‚°è¨­è¨ˆ
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒ¬ãƒ™ãƒ«4ï¼šä¸Šç´šè€…ï¼ˆ6ãƒ¶æœˆä»¥é™ï¼‰
- [ ] æ©Ÿæ¢°å­¦ç¿’ã¨ã®é€£æº
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬
- [ ] A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ
- [ ] ãƒãƒ¼ãƒ  ãƒªãƒ¼ãƒ‰

## ğŸ’¡ ãŠã™ã™ã‚å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

### æŠ€è¡“æ›¸
- ã€Œæ™‚ç³»åˆ—è§£æã€- æ²–æœ¬ç«œç¾©
- ã€ŒTableau ã§ã‚ã‹ã‚‹ï¼ã‚ãŸã‚‰ã—ã„BIã€
- ã€ŒSQLã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³ã€

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’
- Tableau Publicï¼ˆç„¡æ–™ç·´ç¿’ç’°å¢ƒï¼‰
- Courseraã€ŒTime Series Forecastingã€
- Udemyã€ŒSQLå®Ÿè·µã‚³ãƒ¼ã‚¹ã€

### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
- Tableau User Group
- ãƒ‡ãƒ¼ã‚¿åˆ†æå‹‰å¼·ä¼š
- GitHub ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

## ğŸ‰ ã¾ã¨ã‚

æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒTableauäºˆæ¸¬ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹ãƒã‚¤ãƒ³ãƒˆï¼š

**æŠ€è¡“é¢**
- ğŸ“Š **ãƒ‡ãƒ¼ã‚¿å“è³ª**ã‚’æœ€å„ªå…ˆã«è€ƒãˆã‚‹
- âš¡ **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**ã‚’æ„è­˜ã—ãŸè¨­è¨ˆ
- ğŸ”„ **è‡ªå‹•åŒ–**ã§ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚¨ãƒ©ãƒ¼ã‚’å‰Šæ¸›
- ğŸ“ˆ **ç›£è¦–**ã§ç¶™ç¶šçš„æ”¹å–„

**ã‚­ãƒ£ãƒªã‚¢é¢**
- ğŸ’¼ **ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤**ã‚’å¸¸ã«æ„è­˜
- ğŸ¤ **ãƒãƒ¼ãƒ é€£æº**ã‚’å¤§åˆ‡ã«ã™ã‚‹
- ğŸ“š **ç¶™ç¶šå­¦ç¿’**ã§æŠ€è¡“ã‚’ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
- ğŸš€ **æŒ‘æˆ¦**ã‚’æã‚Œãšã«æ–°ã—ã„æŠ€è¡“ã«è§¦ã‚Œã‚‹

**ç§ã®ä½“é¨“è«‡**
æœ€åˆã¯ã€Œãªã‚“ã§å£²ä¸Šã®äºˆæ¸¬ãªã‚“ã¦...ã€ã¨æ€ã£ã¦ã„ã¾ã—ãŸãŒã€å®Ÿéš›ã«ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã£ã¦çµŒå–¶é™£ã«ã€Œç´ æ™´ã‚‰ã—ã„ï¼ã€ã¨è¨€ã‚ã‚ŒãŸæ™‚ã®é”æˆæ„Ÿã¯å¿˜ã‚Œã‚‰ã‚Œã¾ã›ã‚“ã€‚

æ–°äººã®çš†ã•ã‚“ã€ä¸€ç·’ã«äºˆæ¸¬åˆ†æã®ãƒ—ãƒ­ã‚’ç›®æŒ‡ã—ã¾ã—ã‚‡ã†ï¼ğŸš€

---

**ã“ã®è¨˜äº‹ãŒå½¹ã«ç«‹ã£ãŸã‚‰ã€ã‚¹ã‚­â¤ï¸ã¨ãƒ•ã‚©ãƒ­ãƒ¼ã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼**

**æŠ€è¡“çš„ãªè³ªå•ãŒã‚ã‚Œã°ã€ã‚³ãƒ¡ãƒ³ãƒˆã§ãŠæ°—è»½ã«ã©ã†ã ğŸ’¬**

#Tableau #æ™‚ç³»åˆ—äºˆæ¸¬ #æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ #ãƒ‡ãƒ¼ã‚¿åˆ†æ #SQL #Python #API #è‡ªå‹•åŒ– #ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ #ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
